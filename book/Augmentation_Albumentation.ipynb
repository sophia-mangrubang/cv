{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjj0-Tr9Gyz9"
      },
      "source": [
        "(lesson-5)=\n",
        "# Creating and Augmenting Datasets\n",
        "\n",
        "## Overview\n",
        "In this lesson, we will focus on building and preparing datasets for deep learning models, discussing the rationale behind dataset creation and augmentation. We will learn how to perform batch processing of images with multiple augmentations, ensure that associated annotation files stay intact during dataset splitting, and how to choose the right augmentation techniques for different use cases. By the end of this lesson, you’ll have the skills to create a robust dataset pipeline.\n",
        "\n",
        "### Learning Objectives\n",
        "\n",
        "By the end of this section, you will:\n",
        "- Understand the rationale behind building datasets for deep learning, including the importance of augmentations and data splits.\n",
        "- Learn how to split datasets into training, validation, and test sets while ensuring associated annotation files (e.g., XML, JSON) remain intact.\n",
        "- Apply **batch processing** techniques to augment large datasets efficiently.\n",
        "- Customize augmentations based on the specific task, such as object detection, segmentation, or classification.\n",
        "- Explore and implement augmentations using **Albumentations**, a fast and flexible image augmentation library.\n",
        "\n",
        "---\n",
        "\n",
        "## Rationale Behind Dataset Creation and Augmentation\n",
        "\n",
        "When training deep learning models, the **quality and diversity of your dataset** is critical. Here are a few reasons why augmenting and preparing datasets properly is important:\n",
        "\n",
        "- **Dataset Variety**: Real-world data is often limited, so augmentations help create variations in the data (rotation, flipping, brightness, etc.) to make models more robust.\n",
        "- **Data Splitting**: Properly splitting the dataset into training, validation, and test sets is important for ensuring that the model generalizes well. Validation sets help tune hyperparameters, while test sets evaluate final performance.\n",
        "- **Task-Specific Requirements**: Depending on the type of task (e.g., image classification vs. object detection), dataset augmentation strategies might differ. Object detection, for example, requires that the bounding box annotations remain consistent with the augmented images.\n",
        "\n",
        "## Introduction to Albumentations\n",
        "\n",
        "In **LA 2.5**, we explored how to manually apply image augmentations using **PIL** and **OpenCV**. While these foundational skills are critical for understanding how image transformations work, we now introduce **Albumentations**, a high-level image augmentation library designed for speed, simplicity, and flexibility.\n",
        "\n",
        "### Why Knowing the Foundations is Still Important\n",
        "\n",
        "Understanding how image augmentations work at a lower level provides key benefits:\n",
        "1. **Detailed Control**: Manual augmentation techniques using **PIL** or **OpenCV** give you full control over how transformations are applied. This is especially important when handling complex or custom tasks that might not be covered by high-level libraries.\n",
        "2. **Customization**: There are situations where highly specific augmentations are needed, such as when working with **custom image formats** or **non-standard data**. Knowing the underlying operations enables you to extend or modify augmentations beyond the capabilities of higher-level tools.\n",
        "3. **Better Debugging**: When a model's performance suffers from specific augmentations, it’s important to know how they work under the hood. Foundational skills help you troubleshoot issues when high-level libraries behave unexpectedly.\n",
        "\n",
        "While detailed control is important, for large-scale datasets like those used in marine science (e.g., images from ROVs, underwater drones, or satellite imagery), **Albumentations** provides a faster, more efficient way to perform bulk augmentations. Let's explore the syntax and key arguments of some Albumentations augmentations that are particularly useful for marine datasets.\n",
        "\n",
        "---\n",
        "\n",
        "### Useful Albumentations Augmentations for Marine Data\n",
        "\n",
        "Below is a list of key augmentations that can be used for marine datasets. Each transformation includes its **syntax**, **arguments**, and use cases.\n",
        "\n",
        ":::{note}\n",
        "Albumemtations is often installed as A to avoid the long (and annoying to say) name\n",
        "\n",
        "```python\n",
        "import albumentations as A\n",
        "```\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Horizontal and Vertical Flips\n",
        "\n",
        "```python\n",
        "A.HorizontalFlip(p=0.5)\n",
        "A.VerticalFlip(p=0.5)\n",
        "```\n",
        "\n",
        "Here, 'p' is the probability of applying the flip\n",
        "\n",
        "**Use Case**: Flipping helps simulate different orientations of marine animals or objects. For example, mobile animals might be seen from different angles due to movement, and stationary organisms and geologic features can benefit from this augmentation due to variations in camera position. Flipping almost always increase variability, making it a very common augmentation.\n",
        "\n",
        "### Random Rotations\n",
        "\n",
        "```python\n",
        "A.Rotate(limit=45, p=0.7)\n",
        "```\n",
        "\n",
        "Here, 'limit' is the Maximum rotation angle (in degrees) in both directions.'p' is the probability of applying the rotation.\n",
        "\n",
        "**Use Case**: When marine cameras tilt or rotate due to currents or vehicle movement, applying random rotations can make models more robust to different camera orientations.\n",
        "\n",
        "### Brightness and Contrast Adjustments\n",
        "\n",
        "```python\n",
        "A.RandomBrightnessContrast(p=0.5)\n",
        "```\n",
        "\n",
        "Here, 'p' is the probability of applying a random amount of brightness or contrast augmentation\n",
        "\n",
        "**Use Case**: Underwater lighting conditions vary dramatically, especially when working with still cam imagery that has a very bright illuminated foreground and a much darker background.\n",
        "\n",
        "### Gaussian Blur\n",
        "\n",
        "```python\n",
        "A.Blur(blur_limit=3, p=0.3)\n",
        "```\n",
        "\n",
        "Here, 'blur_limit' is the maximum kernel size for blurring and 'p' is the probability of applying that blur\n",
        "\n",
        "**Use Case**: Blurring can simulate the effect of water turbidity, where visibility is reduced due to particles in the water. This is particularly useful for deep-sea environments or locations with high sediment. This can also be useful when looking at aerial survey data of marine mammals where varying degrees of their bodies are underwater or above water leading to a blur like effect. Similarly diffuse flows in hydrothermal imagery can show up as an intense blur, adding this augmentation is a good idea to catch classes that are partially obstructed by it.\n",
        "\n",
        "### Gaussian Noise\n",
        "\n",
        "```python\n",
        "A.GaussNoise(var_limit=(10.0, 50.0), p=0.5)\n",
        "```\n",
        "Here, 'var_limit' is the range of variance for the noise and 'p' is the probability of applying that blur\n",
        "\n",
        ":::{note}\n",
        "There is no strict upper or lower limit imposed by the function itself; it's based on the range you define. However, extremely large variance values might lead to very noisy and potentially unusable images. For that reason its good to stick to something like 10-50\n",
        ":::\n",
        "\n",
        "**Use Case**: Similar to Gaussian blur, adding Gaussian noise simulates image degradation in murky waters or low-light environments. This is important for creating a more realistic training dataset in challenging underwater conditions.\n",
        "\n",
        "### Random Crop\n",
        "\n",
        "```python\n",
        "A.RandomCrop(width=128, height=128, p=0.5)\n",
        "```\n",
        "\n",
        "Here, Width and Height are self explanotory and given in pixel value, and 'p' is the probability of applying the crop\n",
        "\n",
        "**Use Case**: Random cropping can simulate the loss of image data, helping the model learn to focus on partial objects or areas. This is especially useful when the camera cannot capture the entire object due to occlusion or framing issues.\n",
        "\n",
        "### Shift, Scale, Rotate\n",
        "\n",
        "```python\n",
        "A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=20, p=0.7)\n",
        "```\n",
        "\n",
        "Here, 'shift_limit' controls the maximum shift as a fraction of image size, 'scale_limit' is the maximum scaling factor, 'rotate_limit' is the maximum rotation angle in degrees, and 'p' is the probability of applying the transformation.\n",
        "\n",
        "**Use Case**: This augmentation is useful for simulating camera movement underwater, where slight shifts and rotations occur due to currents or vehicle navigation. Scaling can help the model handle different sizes of objects, and shifting ensures robustness against changes in object placement.\n",
        "\n",
        "### Random Shadow\n",
        "\n",
        "```python\n",
        "A.RandomShadow(shadow_roi=(0, 0.5, 1, 1), num_shadows_lower=1, num_shadows_upper=2, shadow_dimension=5, p=0.5)\n",
        "```\n",
        "\n",
        "Here, 'shadow_roi' is the region of interest for placing shadows (coordinates as a fraction of the image size), 'num_shadows_lower' and 'num_shadows_upper' control the range for the number of shadows, 'shadow_dimension' determines the size of the shadow, and 'p' is the probability of applying the shadow.\n",
        "\n",
        "**Use Case**: This augmentation can simulate shadows caused by marine structures, plants, or large marine animals. Shadows can introduce varying light conditions, making the model more resilient to different lighting situations in real-world environments.\n",
        "\n",
        "### CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
        "\n",
        "```python\n",
        "A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=0.3)\n",
        "```\n",
        "Here, 'clip_limit' is the threshold for contrast limiting, 'tile_grid_size' is the size of the grid for histogram equalization, and 'p' is the probability of applying the augmentation.\n",
        "\n",
        "**Use Case**: CLAHE is useful for improving image contrast in underwater environments where lighting can be uneven or dim. It enhances details that may otherwise be missed, particularly in images with low contrast, such as deep-sea or low-light environments.\n",
        "\n",
        "### Resize\n",
        "\n",
        "```python\n",
        "A.Resize(height=180, width=180, p=1.0)\n",
        "```\n",
        "\n",
        "Here, 'height' and 'width' specify the target dimensions of the image, and 'p' is the probability of applying the resize (usually set to 1.0 to ensure resizing is applied to every image).\n",
        "\n",
        "**Use Case**: This ensures that all images are resized to a standard size, such as 180x180, making them consistent for training in deep learning models. Resizing is often necessary when working with images of varying resolutions, particularly in datasets with mixed sources like drone imagery or satellite images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qu6TaQA5Gy0A"
      },
      "source": [
        "---\n",
        "\n",
        "## 2. Dataset Splitting and Ensuring Annotation Integrity\n",
        "\n",
        "When splitting a dataset, it's important to ensure that the associated annotation files (such as **bounding boxes** for object detection or **segmentation masks**) remain aligned with the correct images after augmentation and splitting. Common dataset splits include:\n",
        "- **Training set**: Typically 70-80% of the data.\n",
        "- **Validation set**: Typically 10-15% of the data for tuning the model.\n",
        "- **Test set**: The final 10-15% for evaluating model performance.\n",
        "\n",
        "### Example: Splitting a Dataset with Associated Annotations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84TzJXL3Gy0A"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Directory paths\n",
        "image_dir = '/path/to/images'\n",
        "annotation_dir = '/path/to/annotations'\n",
        "train_dir = '/path/to/train'\n",
        "val_dir = '/path/to/val'\n",
        "test_dir = '/path/to/test'\n",
        "\n",
        "# Get image files\n",
        "images = [f for f in os.listdir(image_dir) if f.endswith('.png')]  # Change extension as needed\n",
        "\n",
        "# Split dataset into training, validation, and test sets\n",
        "train_images, val_test_images = train_test_split(images, test_size=0.3, random_state=42)\n",
        "val_images, test_images = train_test_split(val_test_images, test_size=0.5, random_state=42)\n",
        "\n",
        "# Move images and their annotations to respective folders\n",
        "def move_files(image_list, target_dir):\n",
        "    for image in image_list:\n",
        "        # Move image\n",
        "        shutil.move(os.path.join(image_dir, image), os.path.join(target_dir, 'images', image))\n",
        "\n",
        "        # Move associated annotation (assumes annotation has the same name but different extension)\n",
        "        annotation_file = image.replace('.png', '.xml')  # Adjust extension based on annotation type\n",
        "        shutil.move(os.path.join(annotation_dir, annotation_file), os.path.join(target_dir, 'annotations', annotation_file))\n",
        "\n",
        "# Move the split files\n",
        "move_files(train_images, train_dir)\n",
        "move_files(val_images, val_dir)\n",
        "move_files(test_images, test_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL3-n41uGy0B"
      },
      "source": [
        "## 3. Batch Processing and Custom Augmentations\n",
        "\n",
        "Once the dataset is split, you can apply **batch augmentations** to increase the diversity of the dataset. Depending on the task (classification, object detection, segmentation), certain augmentations may be more appropriate than others.\n",
        "\n",
        "### Example: Batch Augmentations with Albumentations (OpenCV backend)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8_yv6cDXGy0B",
        "outputId": "34d337aa-1eab-4523-933e-82c1a408b700",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/path/to/train/images'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-eed4ff3cd6d5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugmented\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Convert back to original range\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0maugment_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-eed4ff3cd6d5>\u001b[0m in \u001b[0;36maugment_images\u001b[0;34m(input_dir, output_dir, transform)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimage_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/path/to/train/images'"
          ]
        }
      ],
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# Define augmentation pipeline\n",
        "transform = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.Rotate(limit=45, p=0.7),\n",
        "    A.RandomBrightnessContrast(p=0.3),\n",
        "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=25, p=0.5),\n",
        "    A.RandomCrop(width=128, height=128),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "# Batch process images in a folder\n",
        "input_dir = '/path/to/train/images'\n",
        "output_dir = '/path/to/augmented/images'\n",
        "\n",
        "def augment_images(input_dir, output_dir, transform):\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    for image_file in os.listdir(input_dir):\n",
        "        image_path = os.path.join(input_dir, image_file)\n",
        "        image = cv2.imread(image_path)\n",
        "\n",
        "        # Apply augmentation\n",
        "        augmented = transform(image=image)['image']\n",
        "\n",
        "        # Save augmented image\n",
        "        output_path = os.path.join(output_dir, image_file)\n",
        "        cv2.imwrite(output_path, augmented.numpy().transpose(1, 2, 0) * 255)  # Convert back to original range\n",
        "\n",
        "augment_images(input_dir, output_dir, transform)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhXqIeHqGy0B"
      },
      "source": [
        "In this example, we use **Albumentations**, a fast and flexible image augmentation library, to apply various transformations to batches of images. The augmentation pipeline includes horizontal flipping, random rotations, brightness and contrast adjustments, and random cropping.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Customizing Augmentations Based on Task\n",
        "\n",
        "Different computer vision tasks require specific augmentations. Here are some augmentation strategies for common tasks:\n",
        "\n",
        "### 4.1 Object Detection\n",
        "\n",
        "When working on object detection tasks, it's important to ensure that the **bounding boxes** are adjusted appropriately with the image augmentations.\n",
        "\n",
        "#### Example: Augmenting Object Detection Data with Bounding Boxes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88CEmOCrGy0B"
      },
      "outputs": [],
      "source": [
        "transform = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.3),\n",
        "    A.Rotate(limit=45, p=0.7),\n",
        "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n",
        "\n",
        "# Example of augmenting an image with a bounding box\n",
        "image = cv2.imread('/path/to/image.png')\n",
        "bboxes = [[100, 150, 200, 250]]  # Example bounding box in PASCAL VOC format\n",
        "class_labels = ['crab']\n",
        "\n",
        "# Apply the augmentations\n",
        "augmented = transform(image=image, bboxes=bboxes, class_labels=class_labels)\n",
        "aug_image = augmented['image']\n",
        "aug_bboxes = augmented['bboxes']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iugg0PxmGy0C"
      },
      "source": [
        "Here, **Albumentations** ensures that bounding boxes are modified along with the image, keeping the spatial relationships intact. The `bbox_params` argument specifies that we are using Pascal VOC format for bounding boxes.\n",
        "\n",
        "### 4.2 Image Segmentation\n",
        "\n",
        "For segmentation tasks, it’s crucial that **segmentation masks** undergo the same augmentations as the corresponding images.\n",
        "\n",
        "#### Example: Augmenting Segmentation Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CPkCTw-Gy0C"
      },
      "outputs": [],
      "source": [
        "transform = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.Rotate(limit=45, p=0.7),\n",
        "    A.RandomBrightnessContrast(p=0.3),\n",
        "])\n",
        "\n",
        "# Augment both image and mask\n",
        "image = cv2.imread('/path/to/image.png')\n",
        "mask = cv2.imread('/path/to/mask.png', 0)  # Load mask as grayscale\n",
        "\n",
        "# Apply the augmentations to both image and mask\n",
        "augmented = transform(image=image, mask=mask)\n",
        "aug_image = augmented['image']\n",
        "aug_mask = augmented['mask']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2gMJNTwGy0C"
      },
      "source": [
        "In this case, both the image and its corresponding segmentation mask are augmented together to ensure that the mask still matches the transformed image.\n",
        "\n",
        "### 4.3 Image Classification\n",
        "\n",
        "For classification tasks, standard augmentations like **random cropping**, **flipping**, and **brightness/contrast adjustments** are useful to improve model generalization.\n",
        "\n",
        "#### Example: Augmenting Image Classification Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjOTnS6AGy0C"
      },
      "outputs": [],
      "source": [
        "transform = A.Compose([\n",
        "    A.RandomCrop(width=128, height=128),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.3),\n",
        "    A.Rotate(limit=45, p=0.7),\n",
        "])\n",
        "\n",
        "# Apply augmentations to the image\n",
        "image = cv2.imread('/path/to/image.png')\n",
        "aug_image = transform(image=image)['image']\n",
        "\n",
        "# Save augmented image\n",
        "cv2.imwrite('/path/to/augmented_image.png', aug_image)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWdwnO5NGy0C"
      },
      "source": [
        "For classification tasks, augmentations focus on changing the appearance and orientation of the image to help the model learn diverse features.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVuCmTPwGy0C"
      },
      "source": [
        "## Interactive Activity: Augmenting a Dataset of Crabs and Fish\n",
        "\n",
        "In this activity, you will create an augmented dataset using **randomcrab.zip** and **randomfish.zip**. Each ZIP file contains 35 images (180x180 pixels) of crabs and fish, respectively. Your task is to use image augmentations to expand the dataset to **400 images of crabs** and **400 images of fish**, ensuring the output images maintain the same file size (180x180) and keep the original file names.\n",
        "\n",
        "You’ll be provided with starter code that loads the images and applies basic augmentations. Your job is to customize the augmentation pipeline and generate the augmented images.\n",
        "\n",
        "---\n",
        "\n",
        "## Instructions\n",
        "\n",
        "1. **Download and extract the ZIP files**:\n",
        "   - **randomcrab.zip** contains 35 images of crabs.\n",
        "     - [Download randomcrab.zip](./assets/randomcrab.zip)\n",
        "\n",
        "   - **randomfish.zip** contains 35 images of fish.\n",
        "     - [Download randomfish.zip](./assets/randomfish.zip)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2. **Augment each dataset**: Your goal is to generate a total of **400 images** for each class (crabs and fish) by applying various transformations (rotation, brightness, flips, etc.).\n",
        "\n",
        "3. **Ensure consistency**: Each output image must:\n",
        "   - Retain its original file name.\n",
        "   - Be the same size as the original (180x180).\n",
        "\n",
        "4. **Save the augmented images** in a directory called **augmented_data/crabs** for crabs and **augmented_data/fish** for fish.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H7GHHLRGy0C"
      },
      "source": [
        "## Starter Code\n",
        "\n",
        "### Extracting and Loading Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KDYAdzJfGy0C",
        "outputId": "0097a2ea-af6e-4566-e140-a6c1c38adc13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "import cv2\n",
        "import albumentations as A\n",
        "\n",
        "# Paths to the ZIP files\n",
        "crab_zip = 'randomcrab.zip'\n",
        "fish_zip = 'randomfish.zip'\n",
        "\n",
        "# Extract ZIP files\n",
        "def extract_zip(file, extract_path):\n",
        "    with zipfile.ZipFile(file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "\n",
        "# Extract the crab and fish images\n",
        "extract_zip(crab_zip, 'crabs/')\n",
        "extract_zip(fish_zip, 'fish/')\n",
        "\n",
        "# Load the images\n",
        "crab_images = [os.path.join('crabs/', f) for f in os.listdir('crabs/') if f.endswith('.png')]\n",
        "fish_images = [os.path.join('fish/', f) for f in os.listdir('fish/') if f.endswith('.png')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yJieUQRGy0D"
      },
      "source": [
        "### Now, you'll define an augmentation pipeline to apply transformations. Choose augmentations that you think make the most sense for this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "q4-SeSOZGy0D"
      },
      "outputs": [],
      "source": [
        "# Define the augmentation pipeline\n",
        "augmentations = A.Compose([\n",
        "     A.HorizontalFlip(p=0.5),\n",
        "     A.Rotate(limit=30, p=0.7),\n",
        "     A.RandomBrightnessContrast(p=0.5),\n",
        "     A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n",
        "     A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=0.3),\n",
        "\n",
        "])\n",
        "\n",
        "# Create the output directories\n",
        "os.makedirs('augmented_data/crabs', exist_ok=True)\n",
        "os.makedirs('augmented_data/fish', exist_ok=True)\n",
        "\n",
        "\n",
        "# Function to apply augmentations and save images\n",
        "def augment_and_save(image_path, output_dir):\n",
        "    # Read the image\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    # Apply augmentations\n",
        "    augmented = augmentations(image=image)['image']\n",
        "\n",
        "    # Get the base filename (e.g., '1.png')\n",
        "    base_filename = os.path.basename(image_path)\n",
        "\n",
        "    # Save the augmented image to the output directory\n",
        "    output_path = os.path.join(output_dir, base_filename)\n",
        "    cv2.imwrite(output_path, augmented)\n",
        "\n",
        "# Apply augmentations to crab images\n",
        "for crab_image in crab_images:\n",
        "    augment_and_save(crab_image, 'augmented_data/crabs')\n",
        "\n",
        "# Apply augmentations to fish images\n",
        "for fish_image in fish_images:\n",
        "    augment_and_save(fish_image, 'augmented_data/fish')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}